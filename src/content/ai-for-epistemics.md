---
date: 2024-09-07
---
AI for epistemics, or perhaps epistemic AI, is a subtopic under AI risk. The argument goes, that we should pursue [[Differential technology development]] on epistemic capabilities, because that would increase our odds of transitioning to a post-AGI world better, and has some direct safety implications as well. The famous post about the topic outlines three instrumental goals:

1. Good norms & practices for AI-as-knowledge-producers
2. Good norms & practices for AI-as-communicators
3. Differentially high epistemic capabilities

Ways to achieve this include direct and indirect strategies. It's a lot about forecasting, having AI systems that have good epistemics = understanding what's happening = being able to forecast the future well, but it's also about communicating reliably, not manipulating humans depending on context or political affiliation, etc. 

One interesting point that the author makes is that we should have many different orgs (with different political backgrounds) creating forecasting AIs, and if the AIs really get good epistemics, the different models should converge.

The article goes quite deep on the implications, concrete project ideas, etc. First time I read it took a WHILE but I might give it another read if I actually get interested in working on this. I'm pretty much interested in AI for forecasting - look at Orare - but not about to work on any of the listed projects at this point in time.

See [[Highlights - What's Important in AI for Epistemics]]. This article, linked to me by both Niki as well as B., has a clear link to our Orare project, and a link to my work with Niki as well, though less direct. {{Omit this last part about personal info and work.}}